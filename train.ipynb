{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/userspace/pva/nsu-ai-taiga_version\n"
     ]
    }
   ],
   "source": [
    "%cd /userspace/pva/nsu-ai-taiga_version\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:41:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-07-21 11:41:03 | INFO | team_code.ONE_PEACE.one_peace.models.components | ****** Import flash_attn.ops.layer_norm fail, please install flash_attn ******\n",
      "2024-07-21 11:41:03 | INFO | team_code.ONE_PEACE.one_peace.models.transformer.multihead_attention | ****** Import memory_efficient_attention fail, please install xFormers ******\n",
      "2024-07-21 11:41:03 | INFO | team_code.ONE_PEACE.one_peace.models.one_peace.one_peace_base | ****** Import memory_efficient_attention fail, please install xFormers ******\n",
      "2024-07-21 11:41:04 | WARNING | matplotlib | Matplotlib created a temporary cache directory at /tmp/matplotlib-9d1tvode because the default path (/home/pva/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "2024-07-21 11:41:04 | INFO | matplotlib.font_manager | generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ user config directory '/home/pva/.config/Ultralytics' is not writeable, defaulting to '/tmp' or CWD.Alternatively you can define a YOLO_CONFIG_DIR environment variable for this path.\n"
     ]
    }
   ],
   "source": [
    "from team_code.generate import setup_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:41:13 | INFO | team_code.generate | Current working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-21 11:41:13 | INFO | team_code.generate | New working directory: /userspace/pva/nsu-ai-taiga_version/team_code/ONE-PEACE\n",
      "2024-07-21 11:41:16 | INFO | team_code.ONE_PEACE.one_peace.tasks.base_task | dictionary: 50264 types\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2024-07-21 11:42:16 | INFO | team_code.ONE_PEACE.one_peace.models.one_peace.one_peace_retrieval | logit_scale not exists, re-initialized\n",
      "2024-07-21 11:42:21 | INFO | team_code.generate | ONE-PEACE model is loaded from the \"/userspace/dra/nsu-ai/team_code/models/one-peace.pt\".\n",
      "2024-07-21 11:42:21 | INFO | team_code.generate | Restored working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-21 11:42:21 | INFO | team_code.generate | The ONE-PEACE model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2024-07-21 11:42:21 | INFO | team_code.generate | The PCA pipeline is loaded. The feature vector size is 300.\n",
      "2024-07-21 11:42:23 | INFO | team_code.generate | 1000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:24 | INFO | team_code.generate | 2000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:25 | INFO | team_code.generate | 3000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:27 | INFO | team_code.generate | 4000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:28 | INFO | team_code.generate | 5000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:29 | INFO | team_code.generate | 6000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:30 | INFO | team_code.generate | 7000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:32 | INFO | team_code.generate | 8000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:33 | INFO | team_code.generate | 9000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:34 | INFO | team_code.generate | 10000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:35 | INFO | team_code.generate | 11000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:36 | INFO | team_code.generate | 12000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:38 | INFO | team_code.generate | 13000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:39 | INFO | team_code.generate | 14000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:40 | INFO | team_code.generate | 15000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:41 | INFO | team_code.generate | 16000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:42 | INFO | team_code.generate | 17000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:43 | INFO | team_code.generate | 18000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:45 | INFO | team_code.generate | 19000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:46 | INFO | team_code.generate | 20000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:47 | INFO | team_code.generate | 21000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:48 | INFO | team_code.generate | 22000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:49 | INFO | team_code.generate | 23000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:50 | INFO | team_code.generate | 24000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:52 | INFO | team_code.generate | 25000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:53 | INFO | team_code.generate | 26000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:54 | INFO | team_code.generate | 27000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:55 | INFO | team_code.generate | 28000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:56 | INFO | team_code.generate | 29000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:57 | INFO | team_code.generate | 30000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:42:59 | INFO | team_code.generate | 31000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:00 | INFO | team_code.generate | 32000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:01 | INFO | team_code.generate | 33000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:02 | INFO | team_code.generate | 34000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:03 | INFO | team_code.generate | 35000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:04 | INFO | team_code.generate | 36000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:05 | INFO | team_code.generate | 37000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:06 | INFO | team_code.generate | 38000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:08 | INFO | team_code.generate | 39000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:09 | INFO | team_code.generate | 40000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:10 | INFO | team_code.generate | 41000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:11 | INFO | team_code.generate | 42000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:12 | INFO | team_code.generate | 43000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:13 | INFO | team_code.generate | 44000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-21 11:43:16 | INFO | team_code.generate | The text corpus with Wikipedia paragraphs is loaded. There are 44020286 paragraphs in this corpus.\n",
      "2024-07-21 11:43:16 | INFO | team_code.generate | The Annoy index for Wikipedia paragraphs is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-21 11:43:18 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: /userspace/dra/nsu-ai/team_code/models/auxiliary_models/sbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea3098ec75b4af6946aed2bfdc9a42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:43:26 | INFO | team_code.generate | The large language model is loaded.\n",
      "2024-07-21 11:43:46 | INFO | team_code.generate | The Ocr model is loaded.\n",
      "2024-07-21 11:43:46 | INFO | team_code.generate | The YOLOv8 model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "2024-07-21 11:43:48 | INFO | team_code.generate | The Translation models are loaded.\n"
     ]
    }
   ],
   "source": [
    "model, processor = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import setup_model_and_tokenizer, load_images, MultimodalModel, DEVICE\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import LlavaNextProcessor\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llava_finetuning_Dataset(Dataset):\n",
    "    def __init__(self, json_file: str, processor: LlavaNextProcessor):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "        return {'text': item['conversations'][0]['value'], 'image': item['image'], 'answer': item['conversations'][1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Collator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, samples):\n",
    "\n",
    "        images = [item['image'] for item in samples]\n",
    "        human_texts = [item['text'] for item in samples]\n",
    "        gpt_texts = [item['answer'] for item in samples]\n",
    "\n",
    "        loaded_images= load_images(images)\n",
    "        \n",
    "        p_inputs = []\n",
    "        for prompt, image in zip(human_texts, loaded_images):\n",
    "            p_inputs.append(self.processor(text=prompt, images=image, return_tensors=\"pt\"))\n",
    "\n",
    "        return {'input': p_inputs, 'label': gpt_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    :param model: модель\n",
    "    :return: list: список линейных слоев, оставленных для тренировки\n",
    "    \"\"\"\n",
    "    cls = torch.nn.Linear  # Use the standard Linear class for LLaVA-NeXT\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names:  # Remove 'lm_head' if it's not applicable for LLaVA-NeXT\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:5000/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "def train(model: MultimodalModel, processor: LlavaNextProcessor, batch_size: int, epochs: int, dir_train_dataset='train_dataset.json', dir_eval_dataset='eval_dataset.json'):\n",
    "    \n",
    "\n",
    "    modules = find_all_linear_names(model.llm) \n",
    "    train_dataset = Llava_finetuning_Dataset(json_file=dir_train_dataset, processor=processor)\n",
    "\n",
    "    eval_dataset = Llava_finetuning_Dataset(json_file=dir_eval_dataset, processor=processor)\n",
    "    collator = Collator(processor)\n",
    "\n",
    "    # Configuration\n",
    "    lora_config = LoraConfig( \n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=modules,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    llm_model = get_peft_model(model.llm, lora_config)\n",
    "    llm_model\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/userspace/pva/train/results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"/userspace/pva/train/logs\",\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=llm_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:48:42 | INFO | peft.tuners.tuners_utils | Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_json_path, eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/validation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, processor, batch_size, epochs, dir_train_dataset, dir_eval_dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[1;32m     11\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig( \n\u001b[1;32m     12\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     13\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m llm_model\n\u001b[1;32m     24\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     25\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/mapping.py:149\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    148\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/peft_model.py:1395\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/peft_model.py:138\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:166\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:372\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    370\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m     )\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/tuners/lora/model.py:223\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    213\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    214\u001b[0m         adapter_name,\n\u001b[1;32m    215\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/peft/tuners/lora/model.py:320\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# no module could be matched\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[0;31mValueError\u001b[0m: Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "train_json_path, eval_dataset = \"/userspace/pva/train/train.json\", \"/userspace/pva/train/validation.json\"\n",
    "train(model, processor, 16, 10, train_json_path, eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
