{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/userspace/pva/nsu-ai-taiga_version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/userspace/pva/nsu-ai-taiga_version'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /userspace/pva/nsu-ai-taiga_version\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ONLYFANS_CFG'] = '/userspace/pva/configs/lava_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import setup_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 10:42:34 | INFO | team_code.generate | Current working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:42:34 | INFO | team_code.generate | New working directory: /userspace/pva/nsu-ai-taiga_version/team_code/ONE-PEACE\n",
      "2024-07-22 10:42:37 | INFO | team_code.ONE_PEACE.one_peace.tasks.base_task | dictionary: 50264 types\n",
      "2024-07-22 10:43:31 | INFO | team_code.ONE_PEACE.one_peace.models.one_peace.one_peace_retrieval | logit_scale not exists, re-initialized\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | ONE-PEACE model is loaded from the \"/userspace/dra/nsu-ai/team_code/models/one-peace.pt\".\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | Restored working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The ONE-PEACE model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The PCA pipeline is loaded. The feature vector size is 300.\n",
      "2024-07-22 10:43:35 | INFO | team_code.generate | 1000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:37 | INFO | team_code.generate | 2000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:38 | INFO | team_code.generate | 3000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:39 | INFO | team_code.generate | 4000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:40 | INFO | team_code.generate | 5000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:41 | INFO | team_code.generate | 6000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:42 | INFO | team_code.generate | 7000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:44 | INFO | team_code.generate | 8000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:45 | INFO | team_code.generate | 9000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:46 | INFO | team_code.generate | 10000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:47 | INFO | team_code.generate | 11000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:48 | INFO | team_code.generate | 12000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:49 | INFO | team_code.generate | 13000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:50 | INFO | team_code.generate | 14000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:51 | INFO | team_code.generate | 15000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:52 | INFO | team_code.generate | 16000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:54 | INFO | team_code.generate | 17000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:55 | INFO | team_code.generate | 18000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:56 | INFO | team_code.generate | 19000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:57 | INFO | team_code.generate | 20000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:58 | INFO | team_code.generate | 21000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:59 | INFO | team_code.generate | 22000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:00 | INFO | team_code.generate | 23000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:01 | INFO | team_code.generate | 24000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:02 | INFO | team_code.generate | 25000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:03 | INFO | team_code.generate | 26000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:05 | INFO | team_code.generate | 27000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:06 | INFO | team_code.generate | 28000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:07 | INFO | team_code.generate | 29000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:08 | INFO | team_code.generate | 30000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:09 | INFO | team_code.generate | 31000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:10 | INFO | team_code.generate | 32000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:12 | INFO | team_code.generate | 33000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:13 | INFO | team_code.generate | 34000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:14 | INFO | team_code.generate | 35000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:15 | INFO | team_code.generate | 36000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:16 | INFO | team_code.generate | 37000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:18 | INFO | team_code.generate | 38000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:19 | INFO | team_code.generate | 39000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:20 | INFO | team_code.generate | 40000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:21 | INFO | team_code.generate | 41000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:23 | INFO | team_code.generate | 42000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:24 | INFO | team_code.generate | 43000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:25 | INFO | team_code.generate | 44000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The text corpus with Wikipedia paragraphs is loaded. There are 44020286 paragraphs in this corpus.\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The Annoy index for Wikipedia paragraphs is loaded.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:44:55 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: /userspace/dra/nsu-ai/team_code/models/auxiliary_models/sbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083cb37a2ab6494ca795052c42ed262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:45:08 | INFO | team_code.generate | The large language model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The Ocr model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The YOLOv8 model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "2024-07-22 10:45:30 | INFO | team_code.generate | The Translation models are loaded.\n"
     ]
    }
   ],
   "source": [
    "model, processor = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import load_images, MultimodalModel\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import LlavaNextProcessor\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llava_finetuning_Dataset(Dataset):\n",
    "    def __init__(self, json_file: str, processor: LlavaNextProcessor):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "        result = {'text': item['conversations'][0]['value'], 'image': item['image'], 'answer': item['conversations'][1]['value']}\n",
    "        print(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Collator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        print(samples)\n",
    "        images = [item['image'] for item in samples]\n",
    "        human_texts = [item['text'] for item in samples]\n",
    "        gpt_texts = [item['answer'] for item in samples]\n",
    "\n",
    "        loaded_images= load_images(images)\n",
    "        \n",
    "        p_inputs = []\n",
    "        for prompt, image in zip(human_texts, loaded_images):\n",
    "            p_inputs.append(self.processor(text=prompt, images=image, return_tensors=\"pt\"))\n",
    "\n",
    "        return {'input': p_inputs, 'label': gpt_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MultimodalModel, processor: LlavaNextProcessor, batch_size: int, epochs: int, dir_train_dataset='train_dataset.json', dir_eval_dataset='eval_dataset.json'):\n",
    "    \n",
    "\n",
    "    # modules = find_all_linear_names(model.llm) \n",
    "    train_dataset = Llava_finetuning_Dataset(json_file=dir_train_dataset, processor=processor)\n",
    "\n",
    "    eval_dataset = Llava_finetuning_Dataset(json_file=dir_eval_dataset, processor=processor)\n",
    "    collator = Collator(processor)\n",
    "\n",
    "    # Configuration\n",
    "    \n",
    "    # lora_config = LoraConfig.from_peft_type(**model.llm.peft_config)\n",
    "    lora_config = model.llm.peft_config\n",
    "\n",
    "    # llm_model = get_peft_model(model.llm, lora_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/userspace/pva/train/results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"/userspace/pva/train/logs\",\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.llm,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/userspace/pva/weights/llava_next', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'base_layer', 'default'}, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)}\n",
      "{'text': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a black and white photo of a living room with a couch, chair, and table. One of Pahlmann’s most outstanding model rooms was the highly publicized “Pahlmann Peruvian” in November 1941. Following Pahlmann’s five-week tour of South America, Lord &amp; Taylor premiered six model rooms featuring modern and antique Peruvian-style furnishings. The most popular presentation of model rooms, Pahlmann Peruvian attracted 20,000 to 30,000 visitors per month. In addition, Pahlmann’s designs were translated into a line of fabric and rugs for F. Schumacher &amp; Co. Furniture for the Estonian SSR reception room of the Council of Ministers and partial décor, 1965. Image has such text: \"0 2\". Please imagine that you have just seen the same. What is this picture comparing? [/INST]', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/dcaa9438-eb90-4291-94d6-d35fc82fa028.jpg', 'answer': 'wallpaper, living rooms, rooms, cats'}\n",
      "[{}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_json_path, eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/validation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[113], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, processor, batch_size, epochs, dir_train_dataset, dir_eval_dataset)\u001b[0m\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m     31\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer_utils.py:812\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    811\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[112], line 7\u001b[0m, in \u001b[0;36mCollator.__call__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(samples)\n\u001b[0;32m----> 7\u001b[0m     images \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      8\u001b[0m     human_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      9\u001b[0m     gpt_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n",
      "Cell \u001b[0;32mIn[112], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(samples)\n\u001b[0;32m----> 7\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      8\u001b[0m     human_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      9\u001b[0m     gpt_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image'"
     ]
    }
   ],
   "source": [
    "train_json_path, eval_dataset = \"/userspace/pva/train/train.json\", \"/userspace/pva/train/validation.json\"\n",
    "train(model, processor, 1, 10, train_json_path, eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
