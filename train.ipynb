{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/userspace/pva/nsu-ai-taiga_version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/userspace/pva/nsu-ai-taiga_version'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /userspace/pva/nsu-ai-taiga_version\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ONLYFANS_CFG'] = '/userspace/pva/configs/lava_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import setup_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 10:42:34 | INFO | team_code.generate | Current working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:42:34 | INFO | team_code.generate | New working directory: /userspace/pva/nsu-ai-taiga_version/team_code/ONE-PEACE\n",
      "2024-07-22 10:42:37 | INFO | team_code.ONE_PEACE.one_peace.tasks.base_task | dictionary: 50264 types\n",
      "2024-07-22 10:43:31 | INFO | team_code.ONE_PEACE.one_peace.models.one_peace.one_peace_retrieval | logit_scale not exists, re-initialized\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | ONE-PEACE model is loaded from the \"/userspace/dra/nsu-ai/team_code/models/one-peace.pt\".\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | Restored working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The ONE-PEACE model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The PCA pipeline is loaded. The feature vector size is 300.\n",
      "2024-07-22 10:43:35 | INFO | team_code.generate | 1000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:37 | INFO | team_code.generate | 2000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:38 | INFO | team_code.generate | 3000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:39 | INFO | team_code.generate | 4000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:40 | INFO | team_code.generate | 5000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:41 | INFO | team_code.generate | 6000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:42 | INFO | team_code.generate | 7000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:44 | INFO | team_code.generate | 8000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:45 | INFO | team_code.generate | 9000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:46 | INFO | team_code.generate | 10000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:47 | INFO | team_code.generate | 11000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:48 | INFO | team_code.generate | 12000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:49 | INFO | team_code.generate | 13000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:50 | INFO | team_code.generate | 14000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:51 | INFO | team_code.generate | 15000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:52 | INFO | team_code.generate | 16000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:54 | INFO | team_code.generate | 17000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:55 | INFO | team_code.generate | 18000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:56 | INFO | team_code.generate | 19000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:57 | INFO | team_code.generate | 20000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:58 | INFO | team_code.generate | 21000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:59 | INFO | team_code.generate | 22000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:00 | INFO | team_code.generate | 23000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:01 | INFO | team_code.generate | 24000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:02 | INFO | team_code.generate | 25000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:03 | INFO | team_code.generate | 26000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:05 | INFO | team_code.generate | 27000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:06 | INFO | team_code.generate | 28000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:07 | INFO | team_code.generate | 29000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:08 | INFO | team_code.generate | 30000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:09 | INFO | team_code.generate | 31000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:10 | INFO | team_code.generate | 32000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:12 | INFO | team_code.generate | 33000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:13 | INFO | team_code.generate | 34000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:14 | INFO | team_code.generate | 35000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:15 | INFO | team_code.generate | 36000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:16 | INFO | team_code.generate | 37000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:18 | INFO | team_code.generate | 38000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:19 | INFO | team_code.generate | 39000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:20 | INFO | team_code.generate | 40000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:21 | INFO | team_code.generate | 41000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:23 | INFO | team_code.generate | 42000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:24 | INFO | team_code.generate | 43000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:25 | INFO | team_code.generate | 44000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The text corpus with Wikipedia paragraphs is loaded. There are 44020286 paragraphs in this corpus.\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The Annoy index for Wikipedia paragraphs is loaded.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:44:55 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: /userspace/dra/nsu-ai/team_code/models/auxiliary_models/sbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083cb37a2ab6494ca795052c42ed262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:45:08 | INFO | team_code.generate | The large language model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The Ocr model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The YOLOv8 model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "2024-07-22 10:45:30 | INFO | team_code.generate | The Translation models are loaded.\n"
     ]
    }
   ],
   "source": [
    "model, processor = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import load_images, MultimodalModel\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import LlavaNextProcessor\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llava_finetuning_Dataset(Dataset):\n",
    "    def __init__(self, json_file: str, processor: LlavaNextProcessor):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "        print(item)\n",
    "        return {'text': item['conversations'][0]['value'], 'image': item['image'], 'answer': item['conversations'][1]['value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Collator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        print(samples)\n",
    "        images = [item['image'] for item in samples]\n",
    "        human_texts = [item['text'] for item in samples]\n",
    "        gpt_texts = [item['answer'] for item in samples]\n",
    "\n",
    "        loaded_images= load_images(images)\n",
    "        \n",
    "        p_inputs = []\n",
    "        for prompt, image in zip(human_texts, loaded_images):\n",
    "            p_inputs.append(self.processor(text=prompt, images=image, return_tensors=\"pt\"))\n",
    "\n",
    "        return {'input': p_inputs, 'label': gpt_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    :param model: модель\n",
    "    :return: list: список линейных слоев, оставленных для тренировки\n",
    "    \"\"\"\n",
    "    cls = torch.nn.Linear  # Use the standard Linear class for LLaVA-NeXT\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names:  # Remove 'lm_head' if it's not applicable for LLaVA-NeXT\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MultimodalModel, processor: LlavaNextProcessor, batch_size: int, epochs: int, dir_train_dataset='train_dataset.json', dir_eval_dataset='eval_dataset.json'):\n",
    "    \n",
    "\n",
    "    # modules = find_all_linear_names(model.llm) \n",
    "    train_dataset = Llava_finetuning_Dataset(json_file=dir_train_dataset, processor=processor)\n",
    "\n",
    "    eval_dataset = Llava_finetuning_Dataset(json_file=dir_eval_dataset, processor=processor)\n",
    "    collator = Collator(processor)\n",
    "\n",
    "    # Configuration\n",
    "    \n",
    "    # lora_config = LoraConfig.from_peft_type(**model.llm.peft_config)\n",
    "    lora_config = model.llm.peft_config\n",
    "    print(lora_config)\n",
    "\n",
    "    # llm_model = get_peft_model(model.llm, lora_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/userspace/pva/train/results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"/userspace/pva/train/logs\",\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.llm,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/userspace/pva/weights/llava_next', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'base_layer', 'default'}, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)}\n",
      "{'id': 'dcaa9438-eb90-4291-94d6-d35fc82fa028', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/dcaa9438-eb90-4291-94d6-d35fc82fa028.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a black and white photo of a living room with a couch, chair, and table. One of Pahlmann’s most outstanding model rooms was the highly publicized “Pahlmann Peruvian” in November 1941. Following Pahlmann’s five-week tour of South America, Lord &amp; Taylor premiered six model rooms featuring modern and antique Peruvian-style furnishings. The most popular presentation of model rooms, Pahlmann Peruvian attracted 20,000 to 30,000 visitors per month. In addition, Pahlmann’s designs were translated into a line of fabric and rugs for F. Schumacher &amp; Co. Furniture for the Estonian SSR reception room of the Council of Ministers and partial décor, 1965. Image has such text: \"0 2\". Please imagine that you have just seen the same. What is this picture comparing? [/INST]'}, {'from': 'gpt', 'value': 'wallpaper, living rooms, rooms, cats'}]}\n",
      "{'id': '05347375-4f71-4883-aab5-534a82461838', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/05347375-4f71-4883-aab5-534a82461838.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a large orange fish kite flying in the sky. The record attempt was part of the Summer Games program run by UNRWA and available to young Palestinians during their summer break from school. The event brought some 6,000 campers in orange uniforms and blue caps to a beach in Gaza\\'s war-torn north, where they released their kites into clear skies. Some kites included the red, green, black and white Palestinian flag. Kite flying in China dates back to several hundred years BC and slowly spread around the world. It is thought to be the earliest example of man-made flight. Leonardo da Vinci\\'s 15th-century dream of flight found expression in several rational designs, but which relied on poor science.. Image has such text: \"1 1\". Please imagine that you have just seen the same. Wat fuels these fllying objects? [/INST]'}, {'from': 'gpt', 'value': 'air, gas, helium, wind'}]}\n",
      "{'id': '23575ea8-efba-4f28-af04-ab0f75d4b592', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/23575ea8-efba-4f28-af04-ab0f75d4b592.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a table with su su su su su su su su su su su su su su su su. Sist bed: att detta tempel må (Final request: that this temple may) \"The Complete Book of Soups and Stews\".. Image has such text: \"0 2\". Please imagine that you have just seen the same. What are the rolled black food items called? [/INST]'}, {'from': 'gpt', 'value': 'sushi'}]}\n",
      "{'id': '30edbd76-c4c3-4e1a-957f-a8ee9833696b', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/30edbd76-c4c3-4e1a-957f-a8ee9833696b.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a man standing in a bedroom next to a bed. A question about the number of bedrooms a household has, as well as the names, gender and birth dates of any overnight guests was criticised as \"bedroom snooping\" by the Conservative Party in opposition. Scene 3: A Bedroom in Rico\\'s Compound, Havana. Image has such text: \"#\". Please imagine that you have just seen the same. In which type of room facility is this young man standing in? [/INST]'}, {'from': 'gpt', 'value': 'dorm, bedroom, dorm room'}]}\n",
      "{'id': '9f2e17a5-1ec6-4dbf-a6a9-2b4cf49ec821', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/9f2e17a5-1ec6-4dbf-a6a9-2b4cf49ec821.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a boat with a flag on the front. The boat was a member of one wolfpack. 4. She was not a pretty boat.. Image has such text: \"0 0\". Please imagine that you have just seen the same. How fast can you you be going? [/INST]'}, {'from': 'gpt', 'value': '35 mph, slow, 40 mph, 30 miles hour'}]}\n",
      "{'id': 'cd08f519-42df-4cda-8749-d1eec0b766d8', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/cd08f519-42df-4cda-8749-d1eec0b766d8.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a red fire hydrant on the sidewalk in front of a building. These incompatibilities have led to well-documented loss of life and buildings, including the Great Boston fire of 1872, the Great Baltimore Fire in 1904, and the Oakland firestorm of 1991. As of 2017, San Francisco still maintains fire hydrants with a size and thread that are incompatible with those used by most or all other nearby fire departments that would respond in mutual aid conditions, such as occurred during the 1989 Loma Prieta earthquake. The Hy-Red Gasoline Station was listed on the National Register of Historic Places in 1983.. Image has such text: \"a member of the\". Please imagine that you have just seen the same. Who traditionally accesses this red item? [/INST]'}, {'from': 'gpt', 'value': 'fire fighter, firefighters, fireman, fire department'}]}\n",
      "{'id': '8556e450-7617-43cd-abaf-84e126937070', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/8556e450-7617-43cd-abaf-84e126937070.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a cobbed street at night. Lights of a City Street. Street prostitution is often illegal, even in jurisdictions that allow other forms of prostitution.. Image has such text: \"MRDOT RECRE-\". Please imagine that you have just seen the same. What is this road comprised of? [/INST]'}, {'from': 'gpt', 'value': 'stone, cobble stone, cobbles, bricks, cobble'}]}\n",
      "{'id': '70f4e3e8-6843-4b34-84b4-97adbe0424cf', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/70f4e3e8-6843-4b34-84b4-97adbe0424cf.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a woman with a suitcase walks down a street. The term \"jaywalking\" was coined in 1915 to label persons crossing a busy street carelessly and becoming a traffic hazard. The term began to imply recklessness or impertinent behavior as the convention became established. The soldier marches purposefully with his rifle sloped over his left shoulder, his stride deliberately over-extended by several inches. He walks home up Hills Road, toward the centre of Cambridge, with a backward glance over his right shoulder along Station Road towards Cambridge railway station. He is bareheaded, holding his helmet in his right hand, which also clasps a rose, with another rose fallen at his feet. He carries a laurel wreath on his rifle, which also encircles a German helmet carried on his backpack as a trophy of war.. Image has such text: \"Print export\". Please imagine that you have just seen the same. In which city are the headquarters of the bank whose logo is shown in the image located? [/INST]'}, {'from': 'gpt', 'value': 'manhattan, new york'}]}\n",
      "{'id': '20c4a02a-b6cd-4305-9560-86788ca1be8a', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/20c4a02a-b6cd-4305-9560-86788ca1be8a.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a statue of a boy sitting on a bench. Surmounting the plinth is the little digger statue. The soldier stands at ease with his head slightly bowed and his rifle reversed resting on his left boot. The hands are crossed and resting on the rifle butt. He is supported by a tree trunk located behind and to his right side. The sculpture of Lennon is currently not wearing his signature round-lens glasses, which have been stolen, or vandalized, several times. However, during the day, a security guard can be found sitting next to the bench, and he will place glasses on the statue if there is a request.. Image has such text: \"Santa Fe\". Please imagine that you have just seen the same. What is this woman made out of? [/INST]'}, {'from': 'gpt', 'value': 'stone, bronze, steel'}]}\n",
      "{'id': '198f22df-8090-44fb-b34c-41a8a9c8e4f1', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/198f22df-8090-44fb-b34c-41a8a9c8e4f1.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. the view from the top of the building. Ontheroofs is an urban exploration and rooftopping photography project conceived by Vitaliy Raskalov () and Vadim Makhorov (), from Ukraine and from Russia respectively. Their main focus of photography is the exploration of high-rise architectural structures like skyscrapers, bridges, historic sites and cathedrals. The team have been notable for their number of free-climbs up famous buildings and monuments around the world. Typically, their videos are uploaded onto YouTube and showcase their ascent and descent. Raskalov and Makhorov appeared as recurring cast in the documentary series \"URBEX – Enter At Your Own Risk\", which launched globally on Red Bull TV on August 1, 2016. Rooftopping and skywalking are the ascents of rooftops, cranes, antennas, smokestacks, etc., usually illegally, to get an adrenaline rush and take selfie photos or videos. Rooftopping differs from skywalking as the latter is mostly about taking panoramic photographs of the scene below, and safety is more important than the thrill. Rooftopping has been especially popular in Russia. Buildering has a similar goal as rooftopping and skywalking (to reach the roof), but involves climbing the building from the outside rather than infiltrating from the inside.. Image has such text: \"1961 62\". Please imagine that you have just seen the same. What is the weather like? [/INST]'}, {'from': 'gpt', 'value': 'sunny, foggy, cold, overcast'}]}\n",
      "{'id': '17849c2c-c09c-4429-b4c2-af6153bb6c76', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/17849c2c-c09c-4429-b4c2-af6153bb6c76.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a man sitting on a bed with a camera. He hired a cameraman, Kevin Flint, via Craigslist a week before beginning his travels in Los Angeles. Flint received a food stipend, so was not completely dependent on Craigslist users. Garner\\'s only possessions during the 31-day experiment were \"a laptop, a new cell phone with a new number he hadn\\'t shared with anyone, a new email address, a passport, toothbrush and the clothes on his back\". All meals, shelter, and transportation would be acquired solely from connections he made on Craigslist. He has travelled to over 55 countries around the world. He is a co-founder of a website DreamJobbing.com.. Image has such text: \"6 1\". Please imagine that you have just seen the same. The man in the orange shirt has a device what is this device used for? [/INST]'}, {'from': 'gpt', 'value': 'photography, filming, video, camera'}]}\n",
      "{'id': '7227f86c-6622-4881-b527-b3713f3dd046', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/7227f86c-6622-4881-b527-b3713f3dd046.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a street with a bus and people walking on it. Movia bus 3A runs through the full length of the street on its way between Nordhavn station in the northeast and Valbyparken in the southwest. As part of the requirements set by the Department for Transport in The Public Service Vehicles (Open Data) (England) Regulations 2020, the Bus Open Data Service set deadlines for operators to provide data.. Image has such text: \"The French Government of Australia\". Please imagine that you have just seen the same. What style of architecture is pictured? [/INST]'}, {'from': 'gpt', 'value': 'modern, gothic, victorian, baroque'}]}\n",
      "{'id': 'e5dd2bad-e939-4255-9f10-c3c42e50fff8', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/e5dd2bad-e939-4255-9f10-c3c42e50fff8.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a stop sign in a tree. Drivers often fail to obey the avenue\\'s stop signs presenting a public safety issue. The term \"Stop\" gradually replaced \"Danger\" for the purpose of identifying the normal position of stop signals in official publications from circa 1927, with both terms mentioned in the rule book issued in that year.. Image has such text: \"0 2\". Please imagine that you have just seen the same. What type of camera lens took this photo? [/INST]'}, {'from': 'gpt', 'value': 'fisheye, fish eye, panaramic'}]}\n",
      "{'id': 'c52e2a74-3998-44c5-b637-ea37f5f0e70e', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/c52e2a74-3998-44c5-b637-ea37f5f0e70e.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a group of people holding cell phones. Victoria Snow, Brittany Scobie, Conrad Sweatman, Erin Karpluk and Diana Ha also appeared. Electronic privacy advocates agreed with Wardlaw that the case was a \"new frontier\" for Fourth Amendment jurisprudence, since personal communications technology had advanced considerably since 1987, intertwining personal and work lives much more closely. It was expected that the court\\'s ruling, though it would only directly affect government workplaces, would affect the private workplace as well.. Image has such text: \"2 References\". Please imagine that you have just seen the same. What kind of phones are being used? [/INST]'}, {'from': 'gpt', 'value': 'flip, blackberry, cell'}]}\n",
      "{'id': 'f7b13241-a425-4e41-8ff4-4b3ce97194a8', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/f7b13241-a425-4e41-8ff4-4b3ce97194a8.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a surfer is riding a wave on the beach. Surfing at Vazon beach on Guernsey\\'s west coast has been a popular sport for many years. Among the numerous surf sites in the city, the following are considered to be the top pick for surfers and tourists:. Image has such text: \"0 2\". Please imagine that you have just seen the same. What kind of board is that? [/INST]'}, {'from': 'gpt', 'value': 'surf, surfing, surfboard'}]}\n",
      "{'id': 'bc33ea8e-507d-46cd-8c04-7a2f118a9c9d', 'image': '/userspace/mma/nsu-ai-taiga_version/Fine_tune_llava/dataset3/images/bc33ea8e-507d-46cd-8c04-7a2f118a9c9d.jpg', 'conversations': [{'from': 'human', 'value': '<s>[INST] You are a useful and friendly assistant with great erudition and developed intelligence. You can keep up a conversation on various topics and even know how to play complex intellectual games. I have just looked at an <image> that probably corresponds to the following text description. a window with a sign on it. Exterior window displays can be used to sell product and entice customers into the store. An eye-catching, innovative window display can promote the brand image. It can be used to advertise. Windows can give consumers and by-passers understanding as to what goods are sold in store They are also an effective way of promoting fashion trends and providing useful information to the target audience. There is an iconic picture of the brothel with a naked women at each window, enticing the clients to enter.. Image has such text: \"2 2\". Please imagine that you have just seen the same. What is the man in that ad riding? [/INST]'}, {'from': 'gpt', 'value': 'board, snowboard, microphone'}]}\n",
      "[{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_json_path, eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/validation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, processor, batch_size, epochs, dir_train_dataset, dir_eval_dataset)\u001b[0m\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m     31\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer_utils.py:812\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    811\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[107], line 7\u001b[0m, in \u001b[0;36mCollator.__call__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(samples)\n\u001b[0;32m----> 7\u001b[0m     images \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      8\u001b[0m     human_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      9\u001b[0m     gpt_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n",
      "Cell \u001b[0;32mIn[107], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(samples)\n\u001b[0;32m----> 7\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      8\u001b[0m     human_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m      9\u001b[0m     gpt_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image'"
     ]
    }
   ],
   "source": [
    "train_json_path, eval_dataset = \"/userspace/pva/train/train.json\", \"/userspace/pva/train/validation.json\"\n",
    "train(model, processor, 16, 10, train_json_path, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/userspace/pva/weights/llava_next', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'base_layer', 'default'}, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)}\n"
     ]
    }
   ],
   "source": [
    "print(model.llm.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64101"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
