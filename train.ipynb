{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/userspace/pva/nsu-ai-taiga_version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/userspace/pva/nsu-ai-taiga_version'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /userspace/pva/nsu-ai-taiga_version\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ONLYFANS_CFG'] = '/userspace/pva/configs/lava_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import setup_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 10:42:34 | INFO | team_code.generate | Current working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:42:34 | INFO | team_code.generate | New working directory: /userspace/pva/nsu-ai-taiga_version/team_code/ONE-PEACE\n",
      "2024-07-22 10:42:37 | INFO | team_code.ONE_PEACE.one_peace.tasks.base_task | dictionary: 50264 types\n",
      "2024-07-22 10:43:31 | INFO | team_code.ONE_PEACE.one_peace.models.one_peace.one_peace_retrieval | logit_scale not exists, re-initialized\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | ONE-PEACE model is loaded from the \"/userspace/dra/nsu-ai/team_code/models/one-peace.pt\".\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | Restored working directory: /userspace/pva/nsu-ai-taiga_version\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The ONE-PEACE model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2024-07-22 10:43:34 | INFO | team_code.generate | The PCA pipeline is loaded. The feature vector size is 300.\n",
      "2024-07-22 10:43:35 | INFO | team_code.generate | 1000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:37 | INFO | team_code.generate | 2000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:38 | INFO | team_code.generate | 3000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:39 | INFO | team_code.generate | 4000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:40 | INFO | team_code.generate | 5000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:41 | INFO | team_code.generate | 6000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:42 | INFO | team_code.generate | 7000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:44 | INFO | team_code.generate | 8000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:45 | INFO | team_code.generate | 9000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:46 | INFO | team_code.generate | 10000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:47 | INFO | team_code.generate | 11000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:48 | INFO | team_code.generate | 12000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:49 | INFO | team_code.generate | 13000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:50 | INFO | team_code.generate | 14000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:51 | INFO | team_code.generate | 15000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:52 | INFO | team_code.generate | 16000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:54 | INFO | team_code.generate | 17000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:55 | INFO | team_code.generate | 18000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:56 | INFO | team_code.generate | 19000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:57 | INFO | team_code.generate | 20000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:58 | INFO | team_code.generate | 21000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:43:59 | INFO | team_code.generate | 22000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:00 | INFO | team_code.generate | 23000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:01 | INFO | team_code.generate | 24000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:02 | INFO | team_code.generate | 25000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:03 | INFO | team_code.generate | 26000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:05 | INFO | team_code.generate | 27000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:06 | INFO | team_code.generate | 28000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:07 | INFO | team_code.generate | 29000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:08 | INFO | team_code.generate | 30000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:09 | INFO | team_code.generate | 31000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:10 | INFO | team_code.generate | 32000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:12 | INFO | team_code.generate | 33000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:13 | INFO | team_code.generate | 34000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:14 | INFO | team_code.generate | 35000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:15 | INFO | team_code.generate | 36000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:16 | INFO | team_code.generate | 37000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:18 | INFO | team_code.generate | 38000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:19 | INFO | team_code.generate | 39000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:20 | INFO | team_code.generate | 40000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:21 | INFO | team_code.generate | 41000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:23 | INFO | team_code.generate | 42000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:24 | INFO | team_code.generate | 43000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:25 | INFO | team_code.generate | 44000000 paragraphs are loaded from the \"/userspace/dra/nsu-ai/team_code/models/en_wiki_paragraphs.txt\".\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The text corpus with Wikipedia paragraphs is loaded. There are 44020286 paragraphs in this corpus.\n",
      "2024-07-22 10:44:49 | INFO | team_code.generate | The Annoy index for Wikipedia paragraphs is loaded.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:44:55 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: /userspace/dra/nsu-ai/team_code/models/auxiliary_models/sbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083cb37a2ab6494ca795052c42ed262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-22 10:45:08 | INFO | team_code.generate | The large language model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The Ocr model is loaded.\n",
      "2024-07-22 10:45:28 | INFO | team_code.generate | The YOLOv8 model is loaded.\n",
      "/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "2024-07-22 10:45:30 | INFO | team_code.generate | The Translation models are loaded.\n"
     ]
    }
   ],
   "source": [
    "model, processor = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_code.generate import load_images, MultimodalModel\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import LlavaNextProcessor\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llava_finetuning_Dataset(Dataset):\n",
    "    def __init__(self, json_file: str, processor: LlavaNextProcessor):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "        # result = {'text': item['conversations'][0]['value'], 'image': item['image'], 'answer': item['conversations'][1]['value']}\n",
    "        # print(result)\n",
    "        # return result\n",
    "\n",
    "        loaded_images= load_images([item['image']])\n",
    "\n",
    "        return {\n",
    "            'input': self.processor(text=item['conversations'][0]['value'], images=loaded_images[0], return_tensors=\"pt\"),\n",
    "            'label': item['conversations'][1]['value']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Collator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        print(samples)\n",
    "        images = [item['image'] for item in samples]\n",
    "        human_texts = [item['text'] for item in samples]\n",
    "        gpt_texts = [item['answer'] for item in samples]\n",
    "\n",
    "        loaded_images= load_images(images)\n",
    "        \n",
    "        p_inputs = []\n",
    "        for prompt, image in zip(human_texts, loaded_images):\n",
    "            p_inputs.append(self.processor(text=prompt, images=image, return_tensors=\"pt\"))\n",
    "\n",
    "        return {'input': p_inputs, 'label': gpt_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MultimodalModel, processor: LlavaNextProcessor, batch_size: int, epochs: int, dir_train_dataset='train_dataset.json', dir_eval_dataset='eval_dataset.json'):\n",
    "    \n",
    "\n",
    "    # modules = find_all_linear_names(model.llm) \n",
    "    train_dataset = Llava_finetuning_Dataset(json_file=dir_train_dataset, processor=processor)\n",
    "\n",
    "    eval_dataset = Llava_finetuning_Dataset(json_file=dir_eval_dataset, processor=processor)\n",
    "    # collator = Collator(processor)\n",
    "\n",
    "    # Configuration\n",
    "    \n",
    "    # lora_config = LoraConfig.from_peft_type(**model.llm.peft_config)\n",
    "    lora_config = model.llm.peft_config\n",
    "\n",
    "    # llm_model = get_peft_model(model.llm, lora_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/userspace/pva/train/results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"/userspace/pva/train/logs\",\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.llm,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # data_collator=collator,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_json_path, eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/userspace/pva/train/validation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, processor, batch_size, epochs, dir_train_dataset, dir_eval_dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# data_collator=collator,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/trainer_utils.py:812\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    811\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m/userspace/pva/miniconda3/envs/py9min/lib/python3.9/site-packages/transformers/data/data_collator.py:141\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    139\u001b[0m     label \u001b[38;5;241m=\u001b[39m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    140\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[0;32m--> 141\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m first \u001b[38;5;129;01mand\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "train_json_path, eval_dataset = \"/userspace/pva/train/train.json\", \"/userspace/pva/train/validation.json\"\n",
    "train(model, processor, 16, 10, train_json_path, eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
